# Import the libraries
import tweepy
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

consumerKey = 'eIm3uZZk3u0iqxEdKj9Lr3Jf6'
consumerSecret = 'guSP4VrYgwVzD0QiaWAMtJoESVD2cnC3GEsTP56ZXWpuXYN5I7'
accessToken = '1391239076708188160-GJ2uRRUuMeWh4rR8JhoQrhoIPl3MOW'
accessTokenSecret = 'Y1fRqLoCm8Pjx12x4Uz3ojFmNtl8ADi93KHqOaS5jPGRO'

# Create the authentication object
authenticate = tweepy.OAuthHandler(consumerKey, consumerSecret) 
    
# Set the access token and access token secret
authenticate.set_access_token(accessToken, accessTokenSecret) 
    
# Creating the API object while passing in auth information
api = tweepy.API(authenticate, wait_on_rate_limit = True)

all_tweets=[]
while True :
  posts = api.user_timeline(screen_name="BillGates",count = 200,lang ="en", tweet_mode="extended")
  if(len(all_tweets)==800):
    break
  all_tweets.extend(posts)
  print('N of tweets downloaded till now {}'.format(len(all_tweets)))

#  Print the last 5 tweets
  #print("Show the 5 recent tweets:\n")
 # i=1
  #for tweet in posts:
   # print(str(i) +') '+ tweet.full_text + '\n')
    #i= i+1
    
  tweet.full_text.encode("utf-8")
  
  userID = "BillGates"
from pandas import DataFrame
outtweets = [[tweet.id_str, 
              tweet.created_at, 
              tweet.favorite_count, 
              tweet.retweet_count, 
              tweet.full_text.encode("utf-8").decode("utf-8")] 
             for idx,tweet in enumerate(all_tweets)]
df = DataFrame(outtweets,columns=["id","created_at","favorite_count","retweet_count", "text"])
df.to_csv('tweets.csv' ,index=False)
df

# Create a dataframe with a column called Tweets
#df = pd.DataFrame([tweet.full_text for tweet in posts], columns=['Tweets'])
# Show the first 5 rows of data
df

# Create a function to clean the tweets
def cleanTxt(text):
 text = re.sub('@[A-Za-z0â€“9]+', '', text) #Removing @mentions
 text = re.sub('#', '', text) # Removing '#' hash tag
 text = re.sub('RT[\s]+', '', text) # Removing RT
 text = re.sub('https?:\/\/\S+', '', text) # Removing hyperlink
 
 return text


# Clean the tweets
df['text'] = df['text'].apply(cleanTxt)

# Show the cleaned tweets
df

# Create a function to get the subjectivity
def getSubjectivity(text):
   return TextBlob(text).sentiment.subjectivity

# Create a function to get the polarity
def getPolarity(text):
   return  TextBlob(text).sentiment.polarity


# Create two new columns 'Subjectivity' & 'Polarity'
df['Subjectivity'] = df['text'].apply(getSubjectivity)
df['Polarity'] = df['text'].apply(getPolarity)

# Show the new dataframe with columns 'Subjectivity' & 'Polarity'
df

# word cloud visualization
allWords = ' '.join([twts for twts in df['text']])
wordCloud = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(allWords)


plt.imshow(wordCloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# Create a function to compute negative (-1), and positive (+1) analysis
def getAnalysis(score):
  if score < 0:
    return 'Negative'
  elif score == 0:
    return 'Neutral'
  else:
    return 'Positive'
df['Analysis'] = df['Polarity'].apply(getAnalysis)
# Show the dataframe
df

# Printing positive tweets 
print('Printing positive tweets:\n')
j=1
sortedDF = df.sort_values(by=['Polarity']) #Sort the tweets
for i in range(0, sortedDF.shape[0] ):
  if( sortedDF['Analysis'][i] == 'Positive'):
    print(str(j) + ') '+ sortedDF['text'][i])
    print()
    j= j+1
    
plt.figure(figsize=(8,6)) 
for i in range(0, df.shape[0]):
  plt.scatter(df["Polarity"][i], df["Subjectivity"][i], color='Blue') 
# plt.scatter(x,y,color)   
plt.title('Sentiment Analysis') 
plt.xlabel('Polarity') 
plt.ylabel('Subjectivity') 
plt.show()

# Print the percentage of positive tweets
ptweets = df[df.Analysis == 'Positive']
ptweets = ptweets['text']
ptweets

round( (ptweets.shape[0] / df.shape[0]) * 100 , 1)

# Print the percentage of negative tweets
ntweets = df[df.Analysis == 'Negative']
ntweets = ntweets['text']
ntweets

round( (ntweets.shape[0] / df.shape[0]) * 100, 1)

df['Analysis'].value_counts()

# Print the percentage of negative tweets
ntweets = df[df.Analysis == 'Negative']
ntweets = ntweets['text']
ntweets

round( (ntweets.shape[0] / df.shape[0]) * 100, 1)

plt.title('Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Counts')
df['Analysis'].value_counts().plot(kind = 'bar')
plt.show()

df['clean_tweet'] = df['text'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))
df.head()

tokenized_tweet = df['clean_tweet'].apply(lambda x: x.split())
tokenized_tweet.head()

# stem the words
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()

tokenized_tweet = tokenized_tweet.apply(lambda sentence: [stemmer.stem(word) for word in sentence])
tokenized_tweet.head()

for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = " ".join(tokenized_tweet[i])
    
df['clean_tweet'] = tokenized_tweet
df.head()

# feature extraction
from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer()
bow = bow_vectorizer.fit_transform(df['clean_tweet'])
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(bow, df['Analysis'], random_state=0)
print(x_train.shape,y_train.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
logreg = LogisticRegression()
logreg.fit(x_train,y_train)
pred = logreg.predict(x_test)
#print('The accuracy of Logistic classifier training set is {:.2f}'.format(logreg.score(x_train,y_train)))
#print('The accuracy of Logistic classifier test set is {:.2f}'.format(logreg.score(x_test,y_test)))
print("accuracy: {}%".format(round(accuracy_score(y_test, pred)*100,2)))
f1_score(y_test, pred,average = 'micro')

model = LogisticRegression()
model.fit(x_train, y_train)
pred = model.predict(x_test)
accuracy_score(y_test,pred)

from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import MultinomialNB

model2 = MultinomialNB()
model2.fit(x_train , y_train)
pred = model2.predict(x_test)
f1_score(y_test,        pred , average = 'micro')
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,pred)
cm

from sklearn.naive_bayes import MultinomialNB

mb = MultinomialNB()
mb.fit(x_train,y_train)
pred = mb.predict(x_test)
#print('The accuracy of mb classifier training set is {:.2f}'.format(mb.score(x_train,y_train)))
print('The accuracy of mb classifier test set is {:.2f}'.format(mb.score(x_test,y_test)))
cm = confusion_matrix(y_test,pred)
cm

from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import plot_confusion_matrix
#pipe = Pipeline([('vect', CountVectorizer()),
  #               ('tfidf', TfidfTransformer()),
   #              ('model', RandomForestClassifier(n_estimators=50, criterion="entropy"))])
random=RandomForestClassifier()
model = random.fit(x_train, y_train)
prediction = model.predict(x_test)
print("accuracy: {}%".format(round(accuracy_score(y_test, prediction)*100,2)))
cm = confusion_matrix(y_test, prediction)
cm

from sklearn.tree import DecisionTreeClassifier
# Vectorizing and applying TF-IDF
#pipe = Pipeline([('vect', CountVectorizer()),
 #                ('tfidf', TfidfTransformer()),
  #               ('model', DecisionTreeClassifier(criterion= 'entropy',
   #                                        max_depth = 20, 
    #                                       splitter='best', 
     #                                      random_state=42))])
decision=DecisionTreeClassifier()
# Fitting the model
model = decision.fit(x_train, y_train)
# Accuracy
prediction = model.predict(x_test)
print("accuracy: {}%".format(round(accuracy_score(y_test, prediction)*100,2)))

from xgboost import XGBClassifier
decision=XGBClassifier()
# Fitting the model
model = decision.fit(x_train, y_train)
# Accuracy
prediction = model.predict(x_test)
print("accuracy: {}%".format(round(accuracy_score(y_test, prediction)*100,2)))
